model_cache_path: '../ckpt' # Path to cache models
dependencies: [text]
save_path: "./processed.jsonl"

data:
  text:
    use_hf: False # Whether to use huggingface_dataset, if used, ignore the local data path below
    dataset_name: 'yahma/alpaca-cleaned'
    dataset_split: 'train'  
    name: 'default' 
    revision: null
    data_path: 'demos/text_process/reasoners/math_5_samples.json'  # Local data path, supports json, jsonl, parquet formats
    formatter: "TextFormatter" # Data loader type
    keys: 'answer' # Key name to be processed, for sft data, it can be specified as ['instruction','input','output']

processors:
  AnswerFormatterFilter:
    type: "default"
  AnswerNgramFilter:
    min_score: 0.5
    max_score: 1.0
    ngrams: 5
  AnswerGroundTruthFilter: 
    compare_method: exact # exact/math_verify/xverify
  AnswerTokenLengthFilter:
    max_answer_token_length: 1024
    tokenizer_dir: '../Qwen2.5-0.5B-Instruct'

  
  